{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 데이터 가져오기"
      ],
      "metadata": {
        "id": "FseNsFo_Mr4V"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RZK-OO4uMidf"
      },
      "outputs": [],
      "source": [
        "# 구글 드라이브에 있는 데이터 가져오기\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. 장르별 데이터 가져오기\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from keras.preprocessing import image # 이미지파일 사용 위한 라이브러리\n",
        "from keras.preprocessing.image import ImageDataGenerator # 이미지 한꺼번에 불러오기\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 데이터 가져오는 객체 생성\n",
        "data_img = ImageDataGenerator(\n",
        "    rescale= 1./255\n",
        "    )\n",
        "\n",
        "# 훈련데이터\n",
        "train_img = data_img.flow_from_directory(\n",
        "    '/content/drive/MyDrive/images_original/train', # 데이터 경로\n",
        "    target_size=(288, 432), # melspectrogram 이미지 하나의 크기\n",
        "    batch_size = 1, # 배치사이즈 1\n",
        "    class_mode='categorical',# 다중 분류\n",
        "    shuffle=True, # 데이터 섞기\n",
        "    classes=['blues','classical','country','disco','hiphop','jazz','metal','pop','reggae','rock'], # 장르명시\n",
        "    subset='training' # flow_from_directory()가 훈련데이터를 로드\n",
        "  )\n",
        "\n",
        "\n",
        "# 테스트데이터\n",
        "test_img_load = ImageDataGenerator(\n",
        "    rescale=1./255\n",
        ")\n",
        "test_img = test_img_load.flow_from_directory(\n",
        "    '/content/drive/MyDrive/images_original/test', # 테스트데이터 경로\n",
        "    target_size=(288, 432), # 데이터 하나 크기\n",
        "    batch_size=1, # 배치사이즈 1\n",
        "    class_mode='categorical', # 다중분류\n",
        "    shuffle=False, # 데이터 섞기\n",
        "    classes=['blues','classical','country','disco','hiphop','jazz','metal','pop','reggae','rock'] # 장르명 명시\n",
        ")\n",
        "\n",
        "# 각 데이터 파일에 대한 softmax값을 출력하기 위한\n",
        "# shuffle 하지 않은 훈련데이터\n",
        "not_shuffle = data_img.flow_from_directory(\n",
        "    '/content/drive/MyDrive/images_original/train', # 데이터 경로\n",
        "    target_size=(288, 432), # melspectrogram 이미지 하나의 크기\n",
        "    batch_size = 1, # 배치사이즈 1\n",
        "    class_mode='categorical',# 다중 분류\n",
        "    shuffle=False, # 데이터 섞기\n",
        "    classes=['blues','classical','country','disco','hiphop','jazz','metal','pop','reggae','rock'] # 장르명시\n",
        "  )\n",
        "\n",
        "\n",
        "# 클래스 별 이미지 개수 확인(chatgpt 활용)\n",
        "import collections # 필요한 라이브러리\n",
        "class_indices = train_img.class_indices # 클래스 부류 불러오기\n",
        "\n",
        "class_counts = collections.Counter(train_img.classes) # 클래스별 이미지 개수 세기\n",
        "print(\"\\n train image 개수\")\n",
        "for class_name, class_index in class_indices.items() : # 출력\n",
        "  print(f\"{class_name}: {class_counts[class_index]} images\")\n",
        "\n",
        "test_cnt = collections.Counter(test_img.classes)\n",
        "print(\"\\n test image 개수\")\n",
        "for class_name, test_index in class_indices.items() :\n",
        "  print(f\"{class_name}: {test_cnt[test_index]} images \")"
      ],
      "metadata": {
        "id": "NP8y3swkMvvr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 훈련데이터 x, y 나누기\n",
        "x_train = []\n",
        "y_train = []\n",
        "\n",
        "# test_img의 samples 속성을 사용하여 전체 샘플 수를 가져옵니다.\n",
        "total_samples_train = train_img.samples\n",
        "print(total_samples_train)\n",
        "\n",
        "# 전체 데이터를 가져옵니다.\n",
        "for i in range(total_samples_train):\n",
        "    img, label = train_img.next()\n",
        "    x_train.append(img)  # img는 배치 크기만큼의 배열이므로, 첫 번째 항목만 추가합니다.\n",
        "    y_train.append(label)  # label도 마찬가지로 첫 번째 항목만 추가합니다.\n",
        "\n",
        "# 리스트를 NumPy 배열로 변환합니다.\n",
        "x_train = np.array(x_train)\n",
        "y_train = np.array(y_train)\n",
        "\n",
        "x_train = np.squeeze(x_train)\n",
        "y_train = np.squeeze(y_train)\n",
        "\n",
        "print(\"squeeze후 x_shape:\", x_train.shape)\n",
        "print(\"squeeze후 y_shape:\", y_train.shape)\n",
        "\n",
        "print(len(x_train))\n",
        "\n",
        "# 테스트데이터 x, y 나누기\n",
        "\n",
        "x_test = []\n",
        "y_test = []\n",
        "x_test_name = []\n",
        "\n",
        "# test_img의 samples 속성을 사용하여 전체 샘플 수를 가져옵니다.\n",
        "total_samples_test = test_img.samples\n",
        "print(total_samples_test)\n",
        "\n",
        "# 전체 데이터를 가져옵니다.\n",
        "for i in range(total_samples_test):\n",
        "    img, label = test_img.next()\n",
        "    x_test.append(img)  # img는 배치 크기만큼의 배열이므로, 첫 번째 항목만 추가합니다.\n",
        "    y_test.append(label)  # label도 마찬가지로 첫 번째 항목만 추가합니다.\n",
        "    x_test_name.append(test_img.filenames[test_img.index_array[i]])\n",
        "\n",
        "# 리스트를 NumPy 배열로 변환합니다.\n",
        "x_test = np.array(x_test)\n",
        "y_test = np.array(y_test)\n",
        "\n",
        "x_test = np.squeeze(x_test)\n",
        "y_test = np.squeeze(y_test)\n",
        "\n",
        "print(x_test.shape)\n",
        "print(y_test.shape)"
      ],
      "metadata": {
        "id": "Vs4OJYv9M2VO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# softmax 출력을 위해 순서대로 가져온 train 데이터\n",
        "\n",
        "# 훈련데이터 x, y 나누기\n",
        "x_not_shuffle = []\n",
        "y_not_shuffle = []\n",
        "not_shuffle_name = []\n",
        "\n",
        "# test_img의 samples 속성을 사용하여 전체 샘플 수를 가져옵니다.\n",
        "total_samples_original = not_shuffle.samples\n",
        "print(total_samples_original)\n",
        "\n",
        "# 전체 데이터를 가져옵니다.\n",
        "for i in range(total_samples_original):\n",
        "    img, label = not_shuffle.next()\n",
        "    x_not_shuffle.append(img)  # img는 배치 크기만큼의 배열이므로, 첫 번째 항목만 추가합니다.\n",
        "    y_not_shuffle.append(label)  # label도 마찬가지로 첫 번째 항목만 추가합니다.\n",
        "    not_shuffle_name.append(not_shuffle.filenames[not_shuffle.index_array[i]])\n",
        "\n",
        "# 리스트를 NumPy 배열로 변환합니다.\n",
        "x_not_shuffle = np.array(x_not_shuffle)\n",
        "y_not_shuffle = np.array(y_not_shuffle)\n",
        "\n",
        "x_not_shuffle = np.squeeze(x_not_shuffle)\n",
        "y_not_shuffle = np.squeeze(y_not_shuffle)\n",
        "\n",
        "print(\"squeeze후 x_shape:\", x_not_shuffle.shape)\n",
        "print(\"squeeze후 y_shape:\", y_not_shuffle.shape)\n",
        "\n",
        "print(len(x_not_shuffle))\n",
        "print(not_shuffle_name)"
      ],
      "metadata": {
        "id": "L9kaAxz_NAQ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CNN 구현\n",
        "Music Genre classification on GTZAN dataset using CNNs\n",
        "\n",
        "github 주소: https://github.com/Hguimaraes/gtzan.keras/blob/master/nbs/1.1-custom_cnn_2d.ipynb\n",
        "\n",
        "레퍼런스 모델"
      ],
      "metadata": {
        "id": "nfOX840FNRzi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. 필요한 라이브러리 불러오기\n",
        "\n",
        "import os\n",
        "import h5py\n",
        "import librosa\n",
        "import itertools\n",
        "from copy import copy\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import OrderedDict\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Add\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Activation\n",
        "from tensorflow.keras.layers import PReLU\n",
        "from tensorflow.keras.layers import Conv2D\n",
        "from tensorflow.keras.layers import MaxPooling2D\n",
        "from tensorflow.keras.layers import AveragePooling2D\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D\n",
        "from tensorflow.keras.layers import GlobalMaxPooling2D\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
        "from sklearn.metrics import f1_score"
      ],
      "metadata": {
        "id": "Jfl_hvY6NViq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#모델 구성"
      ],
      "metadata": {
        "id": "4_owV5lLNaUk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. CNN layer 블록 만드는 함수\n",
        "def conv_block(x, n_filters, pool_size=(2,2)) :\n",
        "  x = Conv2D(n_filters, (3,3), strides=(1,1), padding='same')(x)  # 필터크기 3x3, 보폭 1, 입력크기=출력크기\n",
        "  x = Activation('relu')(x) # 활성화함수로 ReLU 함수 사용\n",
        "  x = MaxPooling2D(pool_size=pool_size, strides=pool_size)(x) # max pooling 적용\n",
        "  x = Dropout(0.5)(x) # 과대적합 방지\n",
        "  return x"
      ],
      "metadata": {
        "id": "1agQBOSxNbh-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_model(input_shape, num_genres):\n",
        "    inpt = Input(shape=input_shape)\n",
        "    x = conv_block(inpt, 16)\n",
        "    x = conv_block(x, 32)\n",
        "    x = conv_block(x, 64)\n",
        "    x = conv_block(x, 128)\n",
        "\n",
        "    # x = conv_block(x, 192)\n",
        "\n",
        "    # Global Pooling and MLP\n",
        "    x = Flatten()(x)\n",
        "    x = Dropout(0.5)(x)\n",
        "    x = Dense(192, activation='relu',\n",
        "              kernel_regularizer=tf.keras.regularizers.l2(0.02))(x)\n",
        "    x = Dropout(0.25)(x)\n",
        "    predictions = Dense(num_genres,\n",
        "                        activation='softmax',\n",
        "                        kernel_regularizer=tf.keras.regularizers.l2(0.02))(x)\n",
        "\n",
        "    model = Model(inputs=inpt, outputs=predictions)\n",
        "    return model\n",
        "\n",
        "model = create_model((288, 432, 3), 10)\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "jTfz5XWwNe8m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss = tf.keras.losses.categorical_crossentropy, # 다중분류, 원핫인코딩.\n",
        "              optimizer = tf.keras.optimizers.Adam(),\n",
        "              metrics = ['accuracy'])\n",
        "\n",
        "# PyTorch의 학습률 감소 기법 중 하나로, val_loss가 더이상 개선되지 않을 때\n",
        "# 학습률을 동적으로 감소시켜 모델의 학습을 돕는 기법\n",
        "# ReduceLROnPlateau에 대한 설명 링크 : https://wikidocs.net/195132\n",
        "reduceLROnPlat = ReduceLROnPlateau(\n",
        "    monitor = 'val_loss',\n",
        "    factor = 0.895, # lr을 감소시킬 비율 (기본값 0.1 = val_loss 개선되지 않을 때 현재 lr에 0.1을 곱하여 감소시킴)\n",
        "    patience = 3, # val_loss 값이 개선되지 않은 상태를 얼마나 허용할 것인지를 설정\n",
        "    verbose = 1, # True : 감소된 lr에 대한 정보를 출력\n",
        "    mode = 'min', # 성능 개선을 어떻게 측정할지 지정. val_loss값이 감소할 때 성능이 개선되었다고 판단.\n",
        "    min_delta = 0.0001,\n",
        "    cooldown = 2, # lr을 감소시킨 후, 새로운 lr을 적용하기 전에 몇 epoch동안 학습을 일시 정지할지 횟수를 지정\n",
        "    min_lr = 1e-5 # lr을 감소시킬 최소값을 지정 (lr이 이 값보다 작아지지 않도록 함)\n",
        ")\n",
        "\n",
        "batch_size = 32\n",
        "steps_per_epoch = np.ceil(len(x_train)/batch_size)\n",
        "\n",
        "history = model.fit(\n",
        "    x_train, y_train,\n",
        "    batch_size=32,\n",
        "    validation_split = 0.2,\n",
        "    epochs=150,\n",
        "    verbose=1,\n",
        "    callbacks=[reduceLROnPlat]\n",
        ")\n",
        "\n",
        "model.save('/content/drive/MyDrive/deep24_model/is_this_best_question.h5')"
      ],
      "metadata": {
        "id": "24I-sEezNgk6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "print(\"test_loss={:.3f} and test_acc = {:.3f}\".format(score[0],score[1]))\n",
        "\n",
        "plt.figure(figsize=(15,7))\n",
        "\n",
        "plt.subplot(1,2,1)\n",
        "plt.plot(history.history['accuracy'], label='train')\n",
        "plt.plot(history.history['val_accuracy'], label='validation')\n",
        "plt.title('Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "plt.plot(history.history['loss'], label='train')\n",
        "plt.plot(history.history['val_loss'], label='validation')\n",
        "plt.title('Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "8Yra2lO0NlvJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 혼동행렬, softmax 출력"
      ],
      "metadata": {
        "id": "cL7rrWaaNnvJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n",
        "def plot_confusion_matrix(cm, classes,\n",
        "                          normalize=False,\n",
        "                          title='Confusion matrix',\n",
        "                          cmap=plt.cm.Blues):\n",
        "    \"\"\"\n",
        "    This function prints and plots the confusion matrix.\n",
        "    Normalization can be applied by setting `normalize=True`.\n",
        "    \"\"\"\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        print(\"Normalized confusion matrix\")\n",
        "    else:\n",
        "        print('Confusion matrix, without normalization')\n",
        "\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    fmt = '.2f' if normalize else 'd'\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, format(cm[i, j], fmt),\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')"
      ],
      "metadata": {
        "id": "BQW_Q6MoNp8-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "genres = {'blues': 0, 'classical': 1, 'country': 2, 'disco': 3, 'hiphop': 4,\n",
        "          'jazz': 5, 'metal': 6, 'pop': 7, 'reggae': 8, 'rock': 9}\n",
        "# train\n",
        "train_predictions_matrix = model.predict(x_not_shuffle)\n",
        "train_preds_matrix = np.argmax(train_predictions_matrix, axis = 1)\n",
        "y_orig = np.argmax(y_not_shuffle, axis = 1)\n",
        "cm = confusion_matrix(train_preds_matrix, y_orig)\n",
        "\n",
        "keys = OrderedDict(sorted(genres.items(), key=lambda t: t[1])).keys()\n",
        "\n",
        "plt.figure(figsize=(10,10))\n",
        "plot_confusion_matrix(cm, keys, normalize=True)"
      ],
      "metadata": {
        "id": "nwb9FnCWNuqv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DNN"
      ],
      "metadata": {
        "id": "epunvIDROqyn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 필요한 라이브러리 임포트\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import LabelEncoder,OneHotEncoder,StandardScaler\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
        "from tensorflow.keras.layers import Dropout, BatchNormalization, Dense\n",
        "from tensorflow.keras.models import Sequential, load_model\n",
        "from sklearn.preprocessing import StandardScaler"
      ],
      "metadata": {
        "id": "dIlTz2JCPyWR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# 데이터 세트 읽어들기\n",
        "data_feature = pd.read_csv('/content/drive/MyDrive/features_30_sec.csv')\n",
        "\n",
        "# 데이터프레임으로 변환\n",
        "data_df = pd.DataFrame(data_feature)\n",
        "\n",
        "# 결측값이 있는 행 삭제\n",
        "data_df = data_df.dropna()\n",
        "data_df = data_df.reset_index(drop=True)\n",
        "print(data_df)"
      ],
      "metadata": {
        "id": "PMZ2VRQzP1i9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 사이킷런을 이용한 라벨인코딩\n",
        "# 참고문헌 : https://dlearner.tistory.com/22\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "data_labels = data_df['label']\n",
        "\n",
        "# 먼저 레이블 인코딩 과정을 거쳐서 문자열에 숫자 하나씩 매핑\n",
        "data_encoder = LabelEncoder()\n",
        "data_encoder.fit(data_labels)\n",
        "data_labels = data_encoder.transform(data_labels)\n",
        "\n",
        "print(\"data_labels:\")\n",
        "print(data_labels)"
      ],
      "metadata": {
        "id": "8Bfu3NKaP3uE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# data : 특성과 레이블 분리\n",
        "X_data30 = data_df.drop(['label', 'filename'], axis=1)\n",
        "y_data30 = data_labels\n",
        "\n",
        "\n",
        "print(X_data30.shape, y_data30.shape)\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(X_data30, y_data30, test_size=0.2, random_state=42)\n",
        "\n",
        "print(x_train.shape, x_test.shape)\n",
        "print(y_train.shape, y_test.shape)\n",
        "\n",
        "# 데이터 정규화\n",
        "ss30 = StandardScaler()\n",
        "ss30.fit(x_train)\n",
        "train_scaled30 = ss30.transform(x_train)\n",
        "test_scaled30 = ss30.transform(x_test)"
      ],
      "metadata": {
        "id": "9-hf8-FeP794"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from keras.models import load_model\n",
        "\n",
        "# Keras 모델 구성\n",
        "model = tf.keras.Sequential()\n",
        "\n",
        "model.add(tf.keras.layers.Dense(1024, activation='relu', input_shape=(58, )))\n",
        "model.add(tf.keras.layers.Dropout(0.5))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(tf.keras.layers.Dense(1024, activation='relu'))\n",
        "model.add(tf.keras.layers.Dropout(0.5))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(tf.keras.layers.Dense(512, activation='relu'))\n",
        "model.add(tf.keras.layers.Dropout(0.5))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(tf.keras.layers.Dense(128, activation='relu'))\n",
        "model.add(tf.keras.layers.Dropout(0.5))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(tf.keras.layers.Dense(10, activation='softmax'))\n",
        "\n",
        "# 옵티마이저 값 설정\n",
        "# 깃허브 참고 : https://github.com/KunalVaidya99/Music-Genre-Classification/blob/master/DNN_Music_Genre_Classification.ipynb\n",
        "Adam = tf.keras.optimizers.Adam(learning_rate=0.0003, beta_1=0.9, beta_2=0.999, epsilon = 1e-08)\n",
        "\n",
        "# 모델 컴파일\n",
        "model.compile(optimizer=Adam, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "reduceLROnPlat = ReduceLROnPlateau(\n",
        "    monitor = 'val_loss',\n",
        "    factor = 0.895, # lr을 감소시킬 비율 (기본값 0.1 = val_loss 개선되지 않을 때 현재 lr에 0.1을 곱하여 감소시킴)\n",
        "    patience = 3, # val_loss 값이 개선되지 않은 상태를 얼마나 허용할 것인지를 설정\n",
        "    verbose = 1, # True : 감소된 lr에 대한 정보를 출력\n",
        "    mode = 'min', # 성능 개선을 어떻게 측정할지 지정. val_loss값이 감소할 때 성능이 개선되었다고 판단.\n",
        "    min_delta = 0.0001,\n",
        "    cooldown = 2, # lr을 감소시킨 후, 새로운 lr을 적용하기 전에 몇 epoch동안 학습을 일시 정지할지 횟수를 지정\n",
        "    min_lr = 1e-5 # lr을 감소시킬 최소값을 지정 (lr이 이 값보다 작아지지 않도록 함)\n",
        ")\n",
        "\n",
        "\n",
        "# 모델 훈련\n",
        "history = model.fit(train_scaled30, y_train, epochs=150, batch_size=32, #64\n",
        "                        validation_split=0.2, callbacks=[reduceLROnPlat])  # 콜백에 추가"
      ],
      "metadata": {
        "id": "9-WrN7YeQA_H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 평가\n",
        "test_loss30, test_accuracy30 = model.evaluate(test_scaled30, y_test, verbose=2)\n",
        "print('Test Loss = {:.5f}'.format(test_loss30))\n",
        "print('Test Accuracy = {:.5f}'.format(test_accuracy30))"
      ],
      "metadata": {
        "id": "zIva7Q3_QCgm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 훈련 손실과 검증 손실 그래프\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "# 훈련 정확도와 검증 정확도 그래프\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "HbH8BD-zQGXd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#혼동행렬 출력"
      ],
      "metadata": {
        "id": "uGx9iP0PQH4H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n",
        "from collections import OrderedDict\n",
        "import itertools\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "def plot_confusion_matrix(cm, classes,\n",
        "                          normalize=False,\n",
        "                          title='Confusion matrix',\n",
        "                          cmap=plt.cm.Blues):\n",
        "    \"\"\"\n",
        "    This function prints and plots the confusion matrix.\n",
        "    Normalization can be applied by setting `normalize=True`.\n",
        "    \"\"\"\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        print(\"Normalized confusion matrix\")\n",
        "    else:\n",
        "        print('Confusion matrix, without normalization')\n",
        "\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    fmt = '.2f' if normalize else 'd'\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, format(cm[i, j], fmt),\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "\n",
        "genres = {'blues': 0, 'classical': 1, 'country': 2, 'disco': 3, 'hiphop': 4,\n",
        "          'jazz': 5, 'metal': 6, 'pop': 7, 'reggae': 8, 'rock': 9}\n",
        "\n",
        "test_predictions_matrix = model.predict(test_scaled30)\n",
        "test_preds_matrix = np.argmax(test_predictions_matrix, axis = 1)\n",
        "y_orig = y_test\n",
        "cm = confusion_matrix(test_preds_matrix, y_orig)\n",
        "\n",
        "keys = OrderedDict(sorted(genres.items(), key=lambda t: t[1])).keys()\n",
        "\n",
        "plt.figure(figsize=(10,10))\n",
        "plot_confusion_matrix(cm, keys, normalize=True)"
      ],
      "metadata": {
        "id": "TiaacPJ8QJ1k"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}