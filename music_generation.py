
"""멜로디 생성 AI.ipynb

Automatically generated by Colab.

"""

import glob
import pickle
import numpy as np
import tensorflow as tf
from music21 import converter, instrument, note, chord, stream
from tensorflow.keras.models import Sequential, load_model
from tensorflow.keras.layers import Dense, Activation, Embedding, Flatten, Lambda, Bidirectional, Reshape, Input, Dropout, Dense
from tensorflow.keras.layers import LSTM, Input, GRU, Conv1D, MaxPooling1D, Bidirectional
from tensorflow.keras import optimizers, metrics
from tensorflow.keras.layers import BatchNormalization as BatchNorm
import np_utils
from keras.callbacks import ModelCheckpoint
from tensorflow.keras.utils import to_categorical

from google.colab import drive
drive.mount('/content/drive')

############################### 참고 코드 #####################################
# 출처: https://inspiringpeople.github.io/data%20analysis/midi-music-data-extraction-using-music21/
# train용 계이름 음표 추출 코드
def get_notes(path): # mid 파일에서 노트 추출
    notes = []
    for file in glob.glob(path):
        midi = converter.parse(file)
        notes_to_parse = None
        try:
            parts = instrument.partitionByInstrument(midi)
        except TypeError:
            print('## 1 {} file occur error.'.format(file))

        if parts:
            print('## 2 {} file has instrument parts'.format(file))
            notes_to_parse = parts.parts[0].recurse()
        else:
            print('## 3 {} file has notes in a flat structure'.format(file))
            notes_to_parse = midi.flat.notes

        for element in notes_to_parse:
            if isinstance(element, note.Note):
                notes.append(str(element.pitch))
            elif isinstance(element, chord.Chord):
                notes.append('.'.join(str(n) for n in element.normalOrder))

    print("음악의 note, chord 개수: \n",len(notes))
    print("음악의 악보 \n", notes)
    return notes
############################### 참고 코드 #####################################


############################### 참고 코드 #####################################
################ test용 계이름 음표 추출 코드
def get_notes_test(path):
    notes_test = []
    for file in glob.glob(path):
        midi = converter.parse(file)
        notes_to_parse = None
        try:
            parts = instrument.partitionByInstrument(midi)
        except TypeError:
            print('1 {} file error.'.format(file))

        if parts:
            print('2 {} has instrument parts'.format(file))
            notes_to_parse = parts.parts[0].recurse()
        else:
            print('3 {} has notes in a flat structure'.format(file))
            notes_to_parse = midi.flat.notes

        for element in notes_to_parse:
            if isinstance(element, note.Note):
                notes_test.append(str(element.pitch))
            elif isinstance(element, chord.Chord):
                notes_test.append('.'.join(str(n) for n in element.normalOrder))

    print("test 음악의 note, chord 개수: \n",len(notes_test))
    print("test 음악의 악보 \n", notes_test)
    return notes_test
############################### 참고코드 #####################################





###### train용 - 추출한 노트 파일 전처리
def preprocess(notes):       # 추출한 노트 파일 전처리
    code = list(set(notes))  # 문자 데이터에서 중복 문자 제거 후 정렬
    code.sort()
    print(len(code))
    dim = len(code)

    code_to_num = {}
    num_to_code = {}
    for i, data in enumerate(code):
        code_to_num[data] = i
    for i, data in enumerate(code):
        num_to_code[i] = data

    total_data = []
    for i in notes:
        total_data.append(code_to_num[i])

    window_size = 50
    trainX, trainY = [], []
    for i in range(0, len(total_data) - window_size):
        trainX.append(total_data[i:i + window_size])
        trainY.append(total_data[i + window_size])

    # shape (61699, 50, 248)
    # shape[0] trainX에 포함된 전체 샘플의 개수
    # shape[1] 각 샘플(시퀀스)의 길이
    # shape[2] 각 문자는 dim 차원의 원핫 벡터로 표현
             # 악보 데이터에서 dim개의 고유한 문자가 있음

    trainX = tf.one_hot(trainX, dim)
    trainY = tf.one_hot(trainY, dim)

    print("\ntrainX shape: ", trainX.shape)
    print("\ntrainY shape: ", trainY.shape)

    return trainX, trainY, dim, total_data, code_to_num, num_to_code

###### test용 - 추출한 노트 파일 전처리
def test_preprocess(test_notes, code_to_num, num_to_code):  # 추출한 노트 파일 전처리

    # test_notes에서 train_data에 없는 문자를 제거
    filtered_test_notes = [note for note in test_notes if note in code_to_num]
    test_data = [code_to_num[note] for note in filtered_test_notes]

    print(test_data)

    window_size = 50
    testX, testY = [], []
    for i in range(0, len(test_data) - window_size):
        testX.append(test_data[i:i + window_size])
        testY.append(test_data[i + window_size])

    testX = tf.one_hot(testX, dim)
    testY = tf.one_hot(testY, dim)

    print("\ntestX shape: ", testX.shape)
    print("\ntestY shape: ", testY.shape)

    return test_data, testX, testY


######모델 만들기
def train_model(trainX, trainY, dim):
    model = Sequential()
    model.add(LSTM(512, input_shape=(trainX.shape[1], dim), return_sequences=True))
    model.add(Dropout(0.3))
    model.add(LSTM(256))
    model.add(Dropout(0.3))
    model.add(Dense(dim, activation='softmax'))
    model.compile(loss='categorical_crossentropy', optimizer='Adam', metrics=['accuracy'])
    model.summary()

    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

    model.fit(trainX, trainY, batch_size=64, epochs=100, verbose=2)
    model.save('/content/drive/MyDrive/Colab Notebooks/mycountrymodel.h5')


############################### 참고 코드 #####################################
### 출처: https://www.tensorflow.org/tutorials/audio/music_generation?hl=ko
################ 출력된 계이름, 음표로 실제 mid 음악 만들기
def create_music(music_notes):
    offset = 0
    output_notes = []
    # 모델에 의해 예측된 값을 바탕으로 노트와 코드(chord) 객체 만들기
    for pattern in music_notes:
        # 출력값 코드(chord) 일 때
        if ('.' in pattern) or pattern.isdigit():
            notes_in_chord = pattern.split('.')
            notes = []
            for current_note in notes_in_chord:
                new_note = note.Note(int(current_note))
                new_note.storedInstrument = instrument.Piano()
                notes.append(new_note)
            new_chord = chord.Chord(notes)
            new_chord.offset = offset
            output_notes.append(new_chord)
        # 출력값이 노트일 때
        else:
            new_note = note.Note(pattern)
            new_note.offset = offset
            new_note.storedInstrument = instrument.Piano()
            output_notes.append(new_note)

        offset += 0.5  #각 반복마다 오프셋을 0.5 씩 증가시켜서, 음이 쌓이는 것을 방지

    return output_notes
############################### 참고 코드 #####################################




############################### 참고 코드 #####################################
################ wav를 mid를 변환하는 코드
### 출처: https://www.tensorflow.org/tutorials/audio/music_generation?hl=ko
import librosa
import pretty_midi
import numpy as np
import os

def wav_to_midi(wav_file, midi_file, duration=30, speed_factor=2.0, pitch_threshold=0.1, min_note_duration=0.02):
    # Ensure the output directory exists
    os.makedirs(os.path.dirname(midi_file), exist_ok=True)

    # Load the first `duration` seconds of the audio file
    y, sr = librosa.load(wav_file, duration=duration)

    # Extract pitches and magnitudes using librosa
    pitches, magnitudes = librosa.core.piptrack(y=y, sr=sr)

    # Create a PrettyMIDI object
    midi = pretty_midi.PrettyMIDI()
    piano = pretty_midi.Instrument(program=pretty_midi.instrument_name_to_program('Acoustic Grand Piano'))

    # Collect pitch and timing information
    previous_pitch = None
    start_time = 0

    for t in range(pitches.shape[1]):
        pitch = pitches[:, t]
        magnitude = magnitudes[:, t]

        # Select the pitch with the highest magnitude above a threshold
        if magnitude.max() > pitch_threshold:
            note = np.argmax(magnitude)
            pitch_val = pitch[note]
            if pitch_val > 0:
                note_number = int(librosa.hz_to_midi(pitch_val))
                # Ensure note_number is within MIDI range
                if 0 <= note_number <= 127:
                    note_velocity = int(min(magnitude.max(), 127))
                    current_time = t * 0.01 * speed_factor

                    if previous_pitch is not None and previous_pitch == note_number:
                        end_time = current_time
                    else:
                        if previous_pitch is not None:
                            duration = end_time - start_time
                            if duration >= min_note_duration:
                                note = pretty_midi.Note(velocity=note_velocity, pitch=previous_pitch, start=start_time, end=end_time)
                                piano.notes.append(note)
                        start_time = current_time
                        previous_pitch = note_number
                        end_time = current_time

    # Add the last note
    if previous_pitch is not None and (end_time - start_time) >= min_note_duration:
        note = pretty_midi.Note(velocity=note_velocity, pitch=previous_pitch, start=start_time, end=end_time)
        piano.notes.append(note)

    midi.instruments.append(piano)
    midi.write(midi_file)

wav_file_path = '/content/drive/MyDrive/Colab Notebooks/classical_music/country.00096.wav'
midi_file_path = '/content/drive/MyDrive/Colab Notebooks/classical_mid/country.mid'  # 원하는 경로로 지정

wav_to_midi(wav_file_path, midi_file_path, duration=15, speed_factor=2.0, pitch_threshold=0.1, min_note_duration=0.02)
############################### 참고 코드 #####################################





notes = get_notes("/content/drive/MyDrive/Colab Notebooks/mid_songs/*.mid")  # 데이터에서 노트 생성
trainX, trainY, dim, total_data, code_to_num, num_to_code = preprocess(notes) # 노트 전처리

train_model(trainX, trainY, dim)  # 모델 학습

mymodel = load_model('/content/drive/MyDrive/Colab Notebooks/mymodel.h5') # 모델 불러오기
mymodel.summary()

####### test data로 모델 평가
test_notes = get_notes_test("/content/drive/MyDrive/Colab Notebooks/country_test/*.mid") # test 음원 불러오기
test_data, testX, testY = test_preprocess(test_notes, code_to_num, num_to_code)  # 음원을 계이름 음표로 전처리

first_code = test_data[0:0+50]  # test 데이터에서, 50개의 데이터 뽑기 => 첫 입력값
first_code = tf.one_hot(first_code, dim)
first_code = tf.expand_dims(first_code, axis=0)
print(first_code)

pred = mymodel.predict(first_code)  # 모델에 넣고, 출력으로 51번째 코드 예측
pred = np.argmax(pred[0])           # 가장 높은 확률은 갖는 예측값 선택
print("예측값 숫자: ", pred)          # 51번째 코드의 모델 예측값
print("예측값 코드: ", num_to_code[pred])
print("실제값 숫자: " , test_data[0+50]) # 51번째 코드의 실제 값과 비교

######## 모델을 통해 생성된 시퀀스와 실제 data시퀀스를 비교하여 정확도 계산
correct_predictions = 0
music = []
predictions = []
true_values = []
for i in range(len(test_data) - 50):  # test_data 길이 - window_size
    pred = mymodel.predict(first_code, verbose=0)  # 첫 입력값 50개로 다음 코드 예측
    pred = np.argmax(pred[0])
    predictions.append(pred)
    true_values.append(test_data[0 + 50 + i])

    if pred == test_data[0 + 50 + i]:
        correct_predictions += 1

    music.append(pred)

    second = first_code.numpy()[0][1:]  # 첫번째 코드를 제외한 두번째 코드~ 마지막 코드가 담김
    one_hot = tf.one_hot(pred, dim)  # 예측값 원핫 인코딩

    first_code = np.vstack([second, one_hot.numpy()])
    first_code = tf.expand_dims(first_code, axis=0)

accuracy = correct_predictions / (len(test_data) - 50)

print(f"Sequence Accuracy: {accuracy:.2f}%")

music_notes = [num_to_code[num] for num in music]
print(music_notes)

output_notes = create_music(music_notes)  # 만들어진 최종 노트를, mid로 잘 변환되도록 살짝 처리
midi_stream = stream.Stream(output_notes)  # 노트를 사용하여 mid 파일 생성
midi_stream.write('midi', fp='country_test5.mid')  # mid 파일 저장


######## train data에서 뽑아서 노래 출력하는 코드
# 첫 입력값 만들기
# test 데이터에서, 중간의 50개의 데이터 뽑기(200번째 ~ 249번째)
first_code = total_data[10:10+50]
first_code = tf.one_hot(first_code, dim)
first_code = tf.expand_dims(first_code, axis=0)
print(first_code)

pred = mymodel.predict(first_code)  # 모델에 데이터(200번째 ~ 249번째)넣고, 출력으로 250번째 코드 예측
pred = np.argmax(pred[0])  # 가장 높은 확률은 갖는 예측값 선택
print("예측값 숫자: ", pred)  # 250번째 코드의 모델 예측값
print("예측값 코드: ", num_to_code[pred])
print("실제값 숫자: " , test_data[10+50]) # 250번째 코드의 실제 값과 비교
'''

'''
music = []
for i in range(500):
    pred = mymodel.predict(first_code, verbose=0)  # 첫 입력값 50개로 다음 코드 예측
    pred = np.argmax(pred[0])

    music.append(pred)

    second = first_code.numpy()[0][1:]  
    one_hot = tf.one_hot(pred, dim)  # 예측값 원핫 인코딩

    first_code = np.vstack([second, one_hot.numpy()])  
    first_code = tf.expand_dims(first_code, axis=0)    

print(music)

music_notes = [num_to_code[num] for num in music]
print(music_notes)

output_notes = create_music(music_notes) # 만들어진 최종 노트를, mid로 잘 변환되도록 살짝 처리
midi_stream = stream.Stream(output_notes)    # 노트를 사용하여 mid파일 생성
midi_stream.write('midi', fp='country_test.mid')  # mid 파일 저장